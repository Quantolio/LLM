{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, datasets, json, transformers, torch, sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "from loguru import logger\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "from peft import TaskType, PeftModel, LoraConfig, get_peft_model, set_peft_model_state_dict, prepare_model_for_kbit_training, prepare_model_for_int8_training\n",
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "\n",
    "from FinNLP.finnlp.benchmarks.fpb import test_fpb\n",
    "from FinNLP.finnlp.benchmarks.fiqa import test_fiqa , add_instructions\n",
    "from FinNLP.finnlp.benchmarks.tfns import test_tfns\n",
    "from FinNLP.finnlp.benchmarks.nwgi import test_nwgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    0:\"negative\",\n",
    "    1:'positive',\n",
    "    2:'neutral',\n",
    "}\n",
    "\n",
    "tfns = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "tfns = tfns['train']\n",
    "tfns = tfns.to_pandas()\n",
    "tfns['label'] = tfns['label'].apply(lambda x:dic[x])\n",
    "tfns['instruction'] = 'What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}.'\n",
    "tfns.columns = ['input', 'output', 'instruction']\n",
    "tfns = datasets.Dataset.from_pandas(tfns)\n",
    "tfns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dataset = datasets.concatenate_datasets([tfns]*2)\n",
    "train_dataset = tmp_dataset\n",
    "print(tmp_dataset.num_rows)\n",
    "\n",
    "all_dataset = train_dataset.shuffle(seed = 42)\n",
    "all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for item in all_dataset.to_pandas().itertuples():\n",
    "    tmp = {}\n",
    "    tmp[\"instruction\"] = item.instruction\n",
    "    tmp[\"input\"] = item.input\n",
    "    tmp[\"output\"] = item.output\n",
    "    data_list.append(tmp)\n",
    "\n",
    "with open(\"data/dataset_new.jsonl\", 'w') as f:\n",
    "    for example in tqdm(data_list, desc=\"formatting..\"):\n",
    "        f.write(json.dumps(format_example(example)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"THUDM/chatglm2-6b\"\n",
    "jsonl_path = \"data/dataset_new.jsonl\"\n",
    "save_path = 'data/dataset_new'\n",
    "max_seq_length = 512\n",
    "skip_overlength = True\n",
    "\n",
    "def preprocess(tokenizer, config, example, max_seq_length):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False)\n",
    "    input_ids = prompt_ids + target_ids + [config.eos_token_id]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "def read_jsonl(path, max_seq_length, skip_overlength=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, trust_remote_code=True, device_map='auto')\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            example = json.loads(line)\n",
    "            feature = preprocess(tokenizer, config, example, max_seq_length)\n",
    "            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
    "                continue\n",
    "            feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n",
    "            yield feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_generator(lambda: read_jsonl(jsonl_path, max_seq_length, skip_overlength))\n",
    "dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='finetuned_model/',\n",
    "        logging_steps = 500,\n",
    "        num_train_epochs = 2,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=1000,\n",
    "        save_steps=500,\n",
    "        fp16=True,\n",
    "        torch_compile = False,\n",
    "        load_best_model_at_end = True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        remove_unused_columns=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.float16\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name,quantization_config=q_config,trust_remote_code=True,device='cuda')\n",
    "model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n",
    "# LoRA\n",
    "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['chatglm']\n",
    "lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM,inference_mode=False,r=8,lora_alpha=32,lora_dropout=0.1,target_modules=target_modules,bias='none',)\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "resume_from_checkpoint = None\n",
    "if resume_from_checkpoint is not None:\n",
    "    checkpoint_name = os.path.join(resume_from_checkpoint, 'pytorch_model.bin')\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(resume_from_checkpoint, 'adapter_model.bin')\n",
    "        resume_from_checkpoint = False\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        logger.info(f'Restarting from {checkpoint_name}')\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        logger.info(f'Checkpoint {checkpoint_name} not found')\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(\"../data/dataset_new\")\n",
    "dataset = dataset.train_test_split(0.2, shuffle=True, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(input_ids=inputs[\"input_ids\"],labels=inputs[\"labels\"]).loss\n",
    "\n",
    "    def prediction_step(self, model: torch.nn.Module, inputs, prediction_loss_only: bool, ignore_keys = None):\n",
    "        with torch.no_grad():\n",
    "            res = model(input_ids=inputs[\"input_ids\"].to(model.device),labels=inputs[\"labels\"].to(model.device)).loss\n",
    "        return (res, None, None)\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        from transformers.trainer import TRAINING_ARGS_NAME\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        saved_params = {k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad}\n",
    "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = ([tokenizer.pad_token_id] * (seq_len - 1) + ids[(seq_len - 1) :] + [tokenizer.pad_token_id] * (longest - ids_l))\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\"input_ids\": input_ids,\"labels\": labels}\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,            \n",
    "    train_dataset=dataset[\"train\"], \n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[TensorBoardCallback(writer)]\n",
    ")\n",
    "trainer.train()\n",
    "writer.close()\n",
    "model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"THUDM/chatglm2-6b\"\n",
    "peft_model = training_args.output_dir\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(base_model, trust_remote_code=True, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "base_model = \"THUDM/chatglm2-6b\"\n",
    "peft_model = \"/content/drive/My Drive/finetuned_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(base_model, trust_remote_code=True, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "res = test_tfns(model, tokenizer, batch_size = batch_size)\n",
    "res = test_fpb(model, tokenizer, batch_size = batch_size)\n",
    "res = test_fiqa(model, tokenizer, prompt_fun = add_instructions, batch_size = batch_size)\n",
    "res = test_nwgi(model, tokenizer, batch_size = batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
